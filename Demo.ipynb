{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1><center>Contextual Forest demo</center></h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is this project?\n",
    "This project contains the code behind the Contextual Forest model, that I designed during [my bachelor's thesis](http://t.ly/bBNd). It includes a demo notebook with\n",
    "code snippets showing the main functionality and informally exposes the ideas behind the model as well as \n",
    "the key points discussed in my bachelor's thesis regarding the task of Word Sense Disambiguation (WSD).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Contextual Forests, as many other language models, operate under the assumption that the meaning of a word in a sentence can\n",
    "be fully determined by it's context (i.e. the other words in the sentence). If one thinks about it for a second that seems like\n",
    "a very reasonable thing to assume considering that this is essentially what humans do when they communicate with each other.\n",
    "\n",
    "Unfortunately, in many situations context has proven to be rather difficult to figure out using even the most advanced techniques. \n",
    "This is a direct consequence of the knowledge-based component of the context, let's illustrate this with an example.\n",
    "\n",
    "Suppose we are given the folowing sentence:\n",
    "\n",
    "<center><i>\"The best Queen songs redefined rock\"</i></center>\n",
    "\n",
    "Now let's focus our attention in the words _Queen_ and _rock_. For us humans, if familiar with 70's rock, it's an incredibly easy and almost automatic process to recognise that we are talking about the british rock band _Queen_ and that _rock_ is a music genre. For a language model instead, this could present\n",
    "a far more challenging situation since both _Queen_ and _rock_ as separate words can be referring to a considerable number of different things (for example a female monarch and a solid aggregate of minerals). \n",
    "\n",
    "So how do language models deal with this problem? One of the key points\n",
    "is that this words appear <u>together</u> in the same sentence. Probabilistically speaking, I could be talking about the \n",
    "Queen Elizabeth II of the United Kingdom but it's highly unlikely since I'm also talking about some concept \n",
    "represented by the word _rock_ which has not been statistically associated with the context of Queen Elizabeth II. \n",
    "From this point it's only natural to wonder how a language model assigns this probabilities and where do the statistics come from. \n",
    "Depending on the answer to this question we can roughly classify language models in two types:\n",
    "\n",
    "* **Context-free models** such as Word2Vec or GloVe, are based on creating a 1:1 mapping between words and vectors (which are usually referred to as word embeddings). Although specifics in the implementation may be different, the general idea for this models consists in training a neural network over a large corpus of text to get a representation that captures semantic properties. For example if we consider the vectors $k, w, m$ corresponding to the words \"king\", \"woman\" and \"man\" respectively, then the vector $q = (k - m) + w$ it's very close to the one assigned to the word \"queen\". This can be extremely useful in many situations but it's rather complicated to solve the WSD problem with this kind of models since the embedding for every word is unique and independent from the context.\n",
    "\n",
    "<center><img src=\"./imgs/glove.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "* **Dynamic-embedding models** like ELMo, BERT and RoBERTa are based on an architecture call \"transformers\" and since their first appearance arround 2018, this kind of models have been at the top of the NLP world. The difference between this models and the context-free ones is that in this kind of models the embedding of a word is actually assigned **depending** on the other words in the context. They also need huge corpus for training and considerably more computational power than the context-free models but the results in the WSD problem are significantly better than in the latest. Let's illustrate this with an example\n",
    "\n",
    "<center><img src=\"./imgs/green.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "Although dynamic-embedding models have a significantly better performance in the WSD problem, there is a key aspect in the training process that they share with the context-free models; they both need a big text corpus. So having this big corpus for training means that with enough computational power and some fancy architectures one can build a pretty decent model entirely based on statistics infered from the corpus (like training masked language models or predicting the next word like the GPT family) and with no real understanding of the language. So what are we looking at when we are face to face with the state of the art models? Language understanding or statistical inference? The answer is somewhere in between, it's clear that language models have mastered the syntax of language but they have a long way to go to be able to understand the subtilities of language (see [this article](https://aclanthology.org/P19-1459.pdf) from Niven and Kao for an example).\n",
    "\n",
    "\n",
    "While I was thinking about how people solve the WSD problem on a daily basis in conversations, I came to the conclusion that the disambiguation process could not be memory-based. In the previous example, we don't know that _Queen_ is a british rock band because one night while we were discussing music in a bar we heard a friend of a friend using the words _Queen_ and _rock_ in the same sentence. We know that _Queen_ is a british rockband and _rock_ is a music genre because the context of each word is, probabilistically speaking, the most consistent option considering the rest of the words in the sentence. Specifically, when we read the first three words (_The best Queen_) and until the next word, in our head, Queen can mean many things, but the moment we read the word _songs_ we think \"Okay we are talking about music so this _Queen_ must be the rock band\". This kind of association is exactly the idea behind Contextual Forests.\n",
    "\n",
    "## Contextual Forest\n",
    "\n",
    "So I wanted to built a disambiguation system through semantical connections using the context from possible meanings of the words in a sentence. I thought that using context for making connections to find common ground between word meanings sounded a lot like spanning nodes in searching algorithms over graphs so I decided to model this expansion process as different Trees (one for each possible meaning) trying to connect with each other. Given a word, to make the model work I needed some kind of context about specific meanings organized in a structure similar to a graph so I decided to use Wikipedia. Unfortunatly, Wikipedia only provides articles about nouns (objects / people / events ...) so due to this impass and time restrictions the implemented version of Contextual Forests only works for disambiguating nouns. Nevertheless, the process is easily scalable if one finds another resource for covering more words.\n",
    "\n",
    "<center><img src=\"./imgs/model_idea.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "### Step by step\n",
    " \n",
    " The first step is identifying nouns in a sentence and finding all possible meanings for every one. This could have been a problem but forunatly Wikipedia has pages specifically designed for this task:\n",
    "\n",
    "<center><img src=\"./imgs/disambiguation.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "\n",
    "After finding possible meanings, it's worth mentioning that a Wikipedia page has a rather large number of links that recursively expanded can yield to a computationally infeasable search problem. So the next thing the algorithm needed was a \"relevance function\" that could evaluate which links to expand in order to quickly find a connection between Trees (this is commonly known in computer science as heuristic function). This heuristic function needed to represent how close two Wikipedia articles are. At first, I thought in just finding common links between articles, but as it turns out, it's not uncommon for two Wikipedia pages that are not related at all to have common links:\n",
    "\n",
    "```python\n",
    "    >>> from ContextualForest import wiki\n",
    "    >>> A = wiki.page('potato').links.keys()\n",
    "    >>> B = wiki.page('Microsoft').links.keys()\n",
    "    >>> len(A & B)\n",
    "    12\n",
    "```\n",
    "\n",
    "What would eventually end up working was the common links over the *relevant links* and this was a crucial step. \n",
    "\n",
    "So how do we define a relevant link? For figuring that out, we need to define what words are relevant in a Wikipedia page. I identified relevant words with the ones uniformly distributed over the text (note that this relevant words don't need to be repeated many times but just a few times every once in a while). \n",
    "\n",
    "<center><img src=\"./imgs/metrics.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "After that I tried to define link relevance as the average relevance of the words composing the link's title. This prooved to be innefective as a relevance metric because using a non-weighted average can be pretty sensitive to outliers resulting in a bias towards links that have the most relevant word as part of the title. To solve this issue I studied the distribution of the relevance score which I discovered that follows Zipf's distribution.\n",
    "\n",
    "<center><img src=\"./imgs/zipf.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "Instead of computing link relevance as a simple average, I defined it as the inverse image of an average over rank positions capturing the decreasing factor on the scoring values to correct the bias.\n",
    "\n",
    "\n",
    "<center><img src=\"./imgs/link_relevance.png\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "At this point, using the Trees we have all the necessary tools to disambiguate context with this non-supervised technique\n",
    "\n",
    "```python\n",
    "    >>> from ContextualForest import contextual_forest\n",
    "    >>> fr = fr = contextual_forest(\"Queen redefined rock with their songs\")\n",
    "    >>> for word, node in fr.dic.items():\n",
    "            possible_meanings = len(disambiguation(word))\n",
    "            if not possible_meanings:\n",
    "                #no disambiguation page\n",
    "                possible_meanings = 1\n",
    "            print(f\"Word: {word}\\t possible meanings: {possible_meanings}\\n Choosen: {node.page.text[:100]} ...\")\n",
    "    \n",
    "```\n",
    "Which prints\n",
    "\n",
    "Word: rock\t possible meanings: 49\n",
    "\n",
    "\n",
    " Choosen: Rock music is a broad genre of popular music that originated as \"rock and roll\" in the United States ...\n",
    "\n",
    "\n",
    "Word: songs\t possible meanings: 1\n",
    "\n",
    "\n",
    " Choosen: A song is a musical composition intended to be performed by the human voice. This is often done at d ...\n",
    "\n",
    "\n",
    "Word: queen\t possible meanings: 40\n",
    "\n",
    "\n",
    " Choosen: Queen are a British rock band formed in London in 1970. Their classic line-up was Freddie Mercury (l ...\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Note that the algortihm is far from perfect and sometimes can fail to disambiguate context but this approach proves that context can be disambiguated by mining some specific pieces of information in graph-based structures (Knowledge Graphs) mimicing a reasoning process instead of training with millions of examples and learning the statistical information needed for disambiguation from them. This idea to put effort in the *how* models learn instead of *how much* data we have to scale our model capabilities is, in my opinion, something that definetly deserves consideration in order to build models that could ultimatly reason like we humans do.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install torch==1.4.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "import flair\n",
    "\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "# init embedding\n",
    "embedding = TransformerWordEmbeddings('bert-large-uncased')\n",
    "\n",
    "sentence1 = Sentence('the grass is green.')\n",
    "sentence2 = Sentence('she has green eyes and light brown hair.')\n",
    "sentence3 = Sentence('when players hit the ball onto the green, they use a putter.')\n",
    "# embed words in sentence\n",
    "embedding.embed(sentence1)\n",
    "embedding.embed(sentence2)\n",
    "embedding.embed(sentence3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Sentence: \"when players hit the ball onto the green , they use a putter .\"   [− Tokens: 14]]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cosine_sim =  lambda x,y : 1 - cosine(x,y)\n",
    "\n",
    "green1 = sentence1[3].embedding\n",
    "green2 = sentence2[2].embedding\n",
    "green3 = sentence3[7].embedding\n",
    "\n",
    "print(f\"Distance 1-3:{cosine_sim(green1, green3)}\") # color - golf\n",
    "print(f\"Distance 1-2:{cosine_sim(green1, green2)}\") # color - color"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Distance 1-3:0.5156915783882141\n",
      "Distance 1-2:0.7878293395042419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ContextualForest import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "A = wiki.page('Gazpacho').links.keys()\n",
    "B = wiki.page('Microsoft').links.keys()\n",
    "len(A & B)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = wiki.page('bohemian rhapsody').text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d = set_relevance(*stem_text(text))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fr = contextual_forest(\"Queen redefined rock with their songs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# Print results\n",
    "for word, node in fr.dic.items():\n",
    "    possible_meanings = len(disambiguation(word))\n",
    "    if not possible_meanings:\n",
    "        #no disambiguation page\n",
    "        possible_meanings = 1\n",
    "    print(f\"Word: {word}\\t possible meanings: {possible_meanings}\\n Choosen: {node.page.text[:100]} ...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word: rock\t possible meanings: 49\n",
      " Choosen: Rock music is a broad genre of popular music that originated as \"rock and roll\" in the United States ...\n",
      "Word: songs\t possible meanings: 1\n",
      " Choosen: A song is a musical composition intended to be performed by the human voice. This is often done at d ...\n",
      "Word: queen\t possible meanings: 40\n",
      " Choosen: Queen are a British rock band formed in London in 1970. Their classic line-up was Freddie Mercury (l ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1 = Forest([\"Cat food\",\"Dog\"])\n",
    "\"\"\"\n",
    "while not f.Q.empty():\n",
    "    sim,u,v,t1,t2 = f.Q.get()\n",
    "    print(\"sim:{},node:{},node:{}\".format(sim,u.page.title,v.page.title))\"\"\"\n",
    "f1.disambiguate()\n",
    "f1.recover_words()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wiki.page('shot (disambiguation)').links.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disambiguation('shot')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 10000000\n",
    "d = {a : None for a in range(N)}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time\n",
    "N in d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time\n",
    "d[N]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"class Forest():\n",
    "    \"\"\" Implementation of the contextual forest main data structure for \n",
    "        context-based semantic disambiguation.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        words : list of str\n",
    "            The keywords to disambiguate\n",
    "        trees : dict\n",
    "            Mapping between ContextualForest.Tree objects and the root nodes\n",
    "            associated with that tree objects.\n",
    "        dic : dict\n",
    "            Mapping between the words provided to disambiguate and root nodes\n",
    "            associated with tree objects formed in the disambiguation process.\n",
    "        connections : dict\n",
    "            Where keys are combinations of two possible trees and values are \n",
    "            boolean indicating wether or not the pair of trees is connected.\n",
    "        Q : queue.PriorityQueue\n",
    "            The priority queue structure to manage connections and expansion order\n",
    "        Methods\n",
    "        -------\n",
    "        disambiguate()\n",
    "            Performs the disambiguation process (forward).\n",
    "        recover_words()\n",
    "            Recovers words synsets once the disambiguation process has completed.\n",
    "    \"\"\"\n",
    "    def __init__(self,words):\n",
    "        self.words = None\n",
    "        self.trees = {}\n",
    "        self.dic = {}\n",
    "        for word in words:\n",
    "            self.dic[word] = None\n",
    "            self.trees[Tree(word)]= None\n",
    "        self.tree_combs = itertools.combinations(self.trees,2) #possible pairs of trees\n",
    "        self.connections = {pair:False for pair in self.tree_combs}\n",
    "        self.Q = pq()\n",
    "        for tree1,tree2 in self.tree_combs:\n",
    "            for u in tree1.to_expand:\n",
    "                for v in tree2.to_expand:\n",
    "                    sim = -u.similarity(v) #negative because pq orders naturally\n",
    "                    if sim == 0:\n",
    "                        continue\n",
    "                    self.Q.put((sim,u,v,tree1,tree2)) # (similarity, node_1, node_2, tree_1, tree_2)\n",
    "\n",
    "    def disambiguate(self):\n",
    "        \"\"\" Performs the forward disambiguation process expanding the nodes till\n",
    "            all trees are connected by a path.\n",
    "        \"\"\"\n",
    "        #while the are connections to check or some tree is not connected\n",
    "        while not all(self.connections.values()) and not self.Q.empty(): \n",
    "            _,u,v,t1,t2 = self.Q.get()\n",
    "            key = (t1,t2) if (t1,t2) in self.connections else (t2,t1) #depends on itertools\n",
    "            while self.connections[key]:\n",
    "                #while key belongs to an already connectyed tree, pop from pq\n",
    "                _,u,v,t1,t2 = self.Q.get()\n",
    "                key  =  (t1,t2) if (t1,t2) in self.connections else (t2,t1)\n",
    "            #expand both nodes\n",
    "            news_t1 = t1.expand_node(u)\n",
    "            news_t2 = t2.expand_node(v)\n",
    "            #check for intersection\n",
    "            if any([True if n in t2.to_expand else False for n in t1.to_expand]):\n",
    "                self.connections[key] = True\n",
    "                if self.trees[t1] == None:\n",
    "                    self.trees[t1] = u.root\n",
    "                if self.trees[t2] == None:\n",
    "                    self.trees[t2] = v.root\n",
    "            else: #no connection, add new nodes\n",
    "                #expansion:\n",
    "                for u in news_t1:\n",
    "                    for v in news_t2:\n",
    "                        sim = -u.similarity(v)\n",
    "                        if sim == 0:\n",
    "                            continue\n",
    "                        self.Q.put((sim,u,v,t1,t2))\n",
    "                           \n",
    "    def recover_words(self):\n",
    "        \"\"\" Recovers the Nodes associated with the disambiguation of every word provided\n",
    "            in the instanziation of the class and stores it in the `dic` attribute.\n",
    "        \"\"\"\n",
    "        found = False\n",
    "        for tree,node in self.trees.items():\n",
    "            for link,page in node.page.links.items():\n",
    "                if tree.word.lower() == link.lower():\n",
    "                    self.dic[tree.word] =  Node(page,1)\n",
    "                    found = True\n",
    "            if not found:\n",
    "                connection = node\n",
    "                while connection.depth != 1:\n",
    "                    connection = connection.parent\n",
    "                self.dic[tree.word] = connection\n",
    "            found = False\n",
    "    \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "23ad5ed7b665870f317a43f19e6aaa05683df10ee402b5398f41f7e11f1aa8db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}